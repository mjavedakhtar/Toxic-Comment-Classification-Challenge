# Toxic Comment Detection

In 2018, Kaggle launched a competition named **Toxic Comment Classification Challenge**, with objective to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than existing models, using a dataset of comments from Wikipedia’s talk page edits. The goal was to make improvements to the current model that would eventually help online discussion become more productive and respectful.